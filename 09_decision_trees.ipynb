{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Decision Trees\n",
    "\n",
    "**Purpose:** Learn and revise **Decision Trees** for classification and regression in Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Decision Tree?\n",
    "\n",
    "A **decision tree** is a model that splits the feature space into regions using **if-then rules**. Each internal node tests one feature (e.g. \"is \\( x_1 \\leq 5 \\)?\"), and each leaf assigns a **prediction** (class label or constant value).\n",
    "\n",
    "- **Classification:** Leaf = majority class; split chosen to maximize **information gain** (e.g. Gini impurity or entropy).\n",
    "- **Regression:** Leaf = mean of \\( y \\) in that region; split chosen to minimize **MSE** (or similar).\n",
    "\n",
    "**Key idea:** No need to scale features; handles non-linearity; interpretable; prone to **overfitting** if grown deep (control with **max_depth**, **min_samples_leaf**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts to Remember\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Gini impurity** | \\( 1 - \\sum_k p_k^2 \\); measure of node \"mixedness\"; split to reduce it. |\n",
    "| **Entropy** | \\( -\\sum_k p_k \\log p_k \\); alternative criterion for splits. |\n",
    "| **max_depth** | Limit tree depth to reduce overfitting. |\n",
    "| **min_samples_leaf** | Minimum samples in a leaf; larger = simpler tree. |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(200, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(model, feature_names=[\"X1\", \"X2\"], class_names=[\"0\", \"1\"], filled=True, rounded=True)\n",
    "plt.title(\"Decision Tree Structure\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **DecisionTreeClassifier** / **DecisionTreeRegressor**; use **max_depth** and **min_samples_leaf** to avoid overfitting.\n",
    "- **plot_tree** visualizes the tree; **feature_importances_** shows which features matter most.\n",
    "- No feature scaling needed; works with mixed feature types (after encoding)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
