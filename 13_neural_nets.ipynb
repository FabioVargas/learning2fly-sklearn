{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Neural Networks (MLP in Scikit-learn)\n",
    "\n",
    "**Purpose:** Learn and revise **Multilayer Perceptron (MLP)** in Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## What is an MLP?\n",
    "\n",
    "**MLPClassifier** / **MLPRegressor** are feedforward neural networks with one or more **hidden layers**. Each layer applies weights and an activation (ReLU, tanh, logistic). The output layer uses softmax (classification) or identity (regression). Training minimizes loss via gradient descent (e.g. Adam).\n",
    "\n",
    "**Key idea:** **hidden_layer_sizes** defines the network (e.g. (100, 50) = two hidden layers). **Scale features**; tune **learning_rate**, **max_iter**, and **alpha** (L2 regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts to Remember\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **hidden_layer_sizes** | Tuple of layer sizes, e.g. (64, 32). |\n",
    "| **activation** | 'relu', 'tanh', 'logistic'; ReLU is default. |\n",
    "| **solver** | 'adam' (default), 'sgd', 'lbfgs'. |\n",
    "| **alpha** | L2 regularization; larger = less overfitting. |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(300, 4)\n",
    "y = (X[:, 0]*X[:, 1] + X[:, 2] > 0).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(64, 32), activation=\"relu\", solver=\"adam\", max_iter=500, random_state=42, alpha=0.01)\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Loss curve (last 5):\", model.loss_curve_[-5:])\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"MLP Training Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **MLPClassifier** / **MLPRegressor**; **hidden_layer_sizes** and **max_iter**; **scale features**.\n",
    "- **loss_curve_** shows training loss; use it to check convergence.\n",
    "- For large problems consider PyTorch/TensorFlow; sklearn MLP is for small to medium tabular data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
