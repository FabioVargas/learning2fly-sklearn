{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gaussian Naive Bayes\n",
    "\n",
    "**Purpose:** Learn and revise **Gaussian Naive Bayes** in Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Gaussian Naive Bayes?\n",
    "\n",
    "**Naive Bayes** classifiers use **Bayes' theorem** and assume features are **conditionally independent** given the class. For **Gaussian Naive Bayes**, we assume that **each feature** (given the class) is normally distributed:\n",
    "\n",
    "\\[\n",
    "P(x_j \\mid y) = \\frac{1}{\\sqrt{2\\pi \\sigma_j^2}} \\exp\\left(-\\frac{(x_j - \\mu_j)^2}{2\\sigma_j^2}\\right)\n",
    "\\]\n",
    "\n",
    "Then:\n",
    "\\[\n",
    "P(y \\mid X) \\propto P(y) \\prod_{j} P(x_j \\mid y)\n",
    "\\]\n",
    "\n",
    "- The model estimates **mean \\( \\mu_j \\)** and **variance \\( \\sigma_j^2 \\)** per feature per class from the training data.\n",
    "- **\"Naive\"** = independence assumption between features; **\"Gaussian\"** = continuous features modeled with a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts to Remember\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Prior** | \\( P(y) \\) — class frequencies (or Laplace-smoothed). |\n",
    "| **Likelihood** | \\( P(x_j \\mid y) \\) — Gaussian with class-specific mean and variance. |\n",
    "| **Independence** | \\( P(X \\mid y) = \\prod_j P(x_j \\mid y) \\) — simplifies estimation. |\n",
    "| **When to use** | Classification with **continuous** features that are roughly bell-shaped per class. |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Two classes with Gaussian-distributed features\n",
    "np.random.seed(42)\n",
    "X0 = np.random.randn(60, 2) + np.array([0, 0])\n",
    "X1 = np.random.randn(60, 2) + np.array([2, 2])\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0]*60 + [1]*60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Class priors:\", model.class_prior_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **GaussianNB** has no hyperparameters to tune (besides optional **var_smoothing** for numerical stability).\n",
    "- Use when features are **continuous** and approximately Gaussian per class.\n",
    "- Fast to train and works well when the independence assumption is not severely violated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
