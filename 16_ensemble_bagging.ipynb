{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Ensemble Learning: Bagging\n",
    "\n",
    "**Purpose:** Learn and revise **Bagging** (Bootstrap Aggregating) in Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Bagging?\n",
    "\n",
    "**Bagging** trains **many copies** of the same **base estimator** (e.g. a decision tree) each on a **different bootstrap sample** of the training set (random sample with replacement). Predictions are made by **averaging** (regression) or **majority voting** (classification).\n",
    "\n",
    "- **Bootstrap:** Each model sees a random subset of rows; typically ~63% of data per sample (rest are \"out-of-bag\").\n",
    "- **Goal:** Reduce **variance** by averaging over many slightly different models; base estimator should be **unstable** (e.g. trees) to benefit.\n",
    "\n",
    "**Key idea:** Random Forest is bagging of trees with extra feature randomness. **BaggingClassifier** / **BaggingRegressor** let you bag **any** base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts to Remember\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **base_estimator** | The model to bag (e.g. DecisionTreeClassifier); default is a tree. |\n",
    "| **n_estimators** | Number of bootstrap models. |\n",
    "| **max_samples** | Fraction or count of samples per bootstrap (default 1.0). |\n",
    "| **oob_score** | If True, compute out-of-bag score (only with max_samples=1.0). |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(300, 4)\n",
    "y = (X[:, 0]**2 + X[:, 1] > 0).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "base = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "model = BaggingClassifier(estimator=base, n_estimators=50, random_state=42, oob_score=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy (test):\", accuracy_score(y_test, y_pred))\n",
    "print(\"OOB score:\", model.oob_score_)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **BaggingClassifier** / **BaggingRegressor**; **estimator** = base model, **n_estimators** = number of bags.\n",
    "- Use **oob_score=True** for a validation estimate without a separate holdout set.\n",
    "- Bagging works best with **high-variance** base estimators (e.g. deep trees)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
