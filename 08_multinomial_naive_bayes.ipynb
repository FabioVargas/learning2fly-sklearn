{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Multinomial Naive Bayes\n",
    "\n",
    "**Purpose:** Learn and revise **Multinomial Naive Bayes** in Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Multinomial Naive Bayes?\n",
    "\n",
    "**Multinomial Naive Bayes** is commonly used for **discrete count data**, especially **text** (e.g. word counts or term frequencies). It assumes features are **counts** and models the probability of counts with a multinomial distribution. For each class \\( y \\) and feature \\( j \\):\n",
    "\n",
    "\\[\n",
    "P(x_j \\mid y) \\propto \\theta_{jy}^{x_j}\n",
    "\\]\n",
    "\n",
    "where \\( \\theta_{jy} \\) is the (smoothed) frequency of feature \\( j \\) in class \\( y \\) (estimated from training data). The likelihood of a document/sample is proportional to:\n",
    "\n",
    "\\[\n",
    "P(X \\mid y) \\propto \\prod_{j} \\theta_{jy}^{x_j}\n",
    "\\]\n",
    "\n",
    "- **When to use:** Text classification (e.g. spam vs ham) with **count** or **tf-idf** features. Features should be **non-negative counts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts to Remember\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **Counts** | Each \\( x_j \\) is a count (e.g. word frequency); model uses these counts. |\n",
    "| **alpha** | Laplace/additive smoothing so unseen words don’t get zero probability. |\n",
    "| **fit_prior** | Whether to learn class priors \\( P(y) \\) from data (True) or use uniform (False). |\n",
    "| **vs Bernoulli** | Multinomial: \"how many times\"; Bernoulli: \"present or absent\" (binary). |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulated count data (e.g. word counts in 4 \"words\", 2 classes)\n",
    "np.random.seed(42)\n",
    "n = 120\n",
    "X = np.random.poisson(lam=2, size=(n, 4)).astype(float)  # count-like, non-negative\n",
    "X += 0.1  # avoid exact zeros if needed\n",
    "y = (X[:, 0] + X[:, 1] > X[:, 2] + X[:, 3]).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = MultinomialNB(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **MultinomialNB** expects **non-negative count** features (e.g. bag-of-words counts or tf-idf).\n",
    "- **alpha**: smoothing; typically 0.01–1.0; prevents zero probabilities for unseen tokens.\n",
    "- Classic choice for **text classification** when using count-based or tf-idf feature vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
