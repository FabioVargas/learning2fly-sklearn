{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. K-Nearest Neighbors (KNN)\n",
    "\n",
    "**Purpose:** Learn and revise **K-Nearest Neighbors** in Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## What is KNN?\n",
    "\n",
    "**K-Nearest Neighbors** is a **instance-based** (lazy) method: it does not learn a compact model but stores the training data. To predict a new point:\n",
    "\n",
    "1. Find the **k** training points **closest** to the new point (by distance, often Euclidean).\n",
    "2. **Classification:** Output the **majority class** among those k neighbors (or weighted by distance).\n",
    "3. **Regression:** Output the **mean** (or weighted mean) of their target values.\n",
    "\n",
    "**Key idea:** No training step; prediction cost grows with data size. **Scale features** so distances are meaningful. **k** controls biasâ€“variance: small k = flexible (noisy); large k = smooth (underfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts to Remember\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **k** | Number of neighbors; odd for binary classification to avoid ties. |\n",
    "| **Distance** | Default: Euclidean; **metric** can be 'manhattan', 'minkowski', etc. |\n",
    "| **weights** | 'uniform' = all k neighbors equal; 'distance' = closer neighbors count more. |\n",
    "| **When to use** | Simple baseline; small to medium data; interpretable (neighbors are the \"explanation\"). |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "X0 = np.random.randn(50, 2) + np.array([0, 0])\n",
    "X1 = np.random.randn(50, 2) + np.array([2, 2])\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0]*50 + [1]*50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "k = 5\n",
    "model = KNeighborsClassifier(n_neighbors=k, weights=\"uniform\")\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "\n",
    "print(f\"K = {k}\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "h = 0.05\n",
    "x_min, x_max = X_train_s[:, 0].min()-0.5, X_train_s[:, 0].max()+0.5\n",
    "y_min, y_max = X_train_s[:, 1].min()-0.5, X_train_s[:, 1].max()+0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X_train_s[:, 0], X_train_s[:, 1], c=y_train, edgecolors=\"k\")\n",
    "plt.xlabel(\"X1\"); plt.ylabel(\"X2\")\n",
    "plt.title(f\"KNN decision boundary (k={k})\")\n",
    "plt.tight_layout(); plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **KNeighborsClassifier** / **KNeighborsRegressor**; **n_neighbors** (k) is the main hyperparameter.\n",
    "- Always **scale features** so distance is not dominated by one variable.\n",
    "- Use **weights='distance'** to weight by inverse distance; tune **k** with cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
