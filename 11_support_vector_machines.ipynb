{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Support Vector Machines (SVM)\n",
    "\n",
    "**Purpose:** Learn and revise **Support Vector Machines** for classification and regression in Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## What is an SVM?\n",
    "\n",
    "**Support Vector Machines** find a **decision boundary** that maximizes the **margin** (distance to the nearest training points, the **support vectors**).\n",
    "\n",
    "- **Linear SVM:** Boundary is a hyperplane \\( w^T x + b = 0 \\); only points near the boundary (support vectors) matter.\n",
    "- **Kernel trick:** Map data to a higher-dimensional space so a linear boundary there becomes non-linear in the original space. Common kernels: **linear**, **RBF** (radial basis function), **polynomial**.\n",
    "- **C:** Regularization; small C = wider margin, more misclassifications allowed; large C = harder margin, risk of overfitting.\n",
    "\n",
    "**Key idea:** Effective in high dimensions; performance depends on **kernel** and **C** (and **gamma** for RBF). **Scale features** before fitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts to Remember\n",
    "\n",
    "| Concept             | Description                                                                                       |\n",
    "| ------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| **Support vectors** | Training points that define the margin; only these affect the model.                              |\n",
    "| **RBF kernel**      | \\( K(x,x') = \\exp(-\\gamma \\|x-x'\\|^2) \\); **gamma** controls \"spread\" (large = complex boundary). |\n",
    "| **C**               | Trade-off between margin size and misclassification; larger C = stricter fit.                     |\n",
    "| **When to use**     | Classification/regression with clear margin; use **StandardScaler** first.                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X0 = np.random.randn(50, 2) + np.array([1, 1])\n",
    "X1 = np.random.randn(50, 2) + np.array([3, 3])\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0]*50 + [1]*50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42)\n",
    "model.fit(X_train_s, y_train)\n",
    "y_pred = model.predict(X_test_s)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Number of support vectors:\", len(model.support_))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.02\n",
    "x_min, x_max = X_train_s[:, 0].min()-0.5, X_train_s[:, 0].max()+0.5\n",
    "y_min, y_max = X_train_s[:, 1].min()-0.5, X_train_s[:, 1].max()+0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X_train_s[:, 0], X_train_s[:, 1], c=y_train, edgecolors=\"k\")\n",
    "plt.scatter(X_train_s[model.support_, 0], X_train_s[model.support_, 1], s=100, facecolors=\"none\", edgecolors=\"red\", linewidths=2, label=\"Support vectors\")\n",
    "plt.legend(); plt.title(\"SVM with RBF kernel\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **SVC** (classification) / **SVR** (regression); **kernel**: 'linear', 'rbf', 'poly'. **Scale features** (StandardScaler).\n",
    "- **C** and **gamma** (for RBF) are main hyperparameters; tune with GridSearchCV.\n",
    "- **support*vectors*** and **n*support*** give the support vectors; useful for interpretation and debugging.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
