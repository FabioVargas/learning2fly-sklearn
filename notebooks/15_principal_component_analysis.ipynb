{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Principal Component Analysis (PCA)\n",
    "\n",
    "**Purpose:** Learn and revise **PCA** for dimensionality reduction in Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## What is PCA?\n",
    "\n",
    "**Principal Component Analysis** finds **linear combinations** of the original features that capture the most **variance**. The first principal component (PC1) is the direction of maximum variance; PC2 is the next, orthogonal to PC1; and so on.\n",
    "\n",
    "- **Eigen decomposition** of the covariance matrix (or SVD of centered data) gives the principal directions and explained variance.\n",
    "- You choose the number of components \\( k \\) (or a target fraction of variance, e.g. 0.95). Transformed data has \\( k \\) dimensions.\n",
    "\n",
    "**Key idea:** **Center** (and usually **scale**) the data before PCA. Use for visualization (e.g. 2D), noise reduction, or as preprocessing before another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts to Remember\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **n_components** | Number of components to keep, or float (e.g. 0.95) for variance ratio. |\n",
    "| **Explained variance** | How much variance each PC captures; **explained_variance_ratio_** sums to 1. |\n",
    "| **Centering** | PCA centers data by default; use **StandardScaler** if features have different scales. |\n",
    "| **When to use** | Dimensionality reduction, visualization, decorrelation, noise reduction. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 5)  # 5 features\n",
    "X[:, 1] = 0.9 * X[:, 0] + 0.1 * np.random.randn(100)  # correlate with feature 0\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_s = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_s)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative:\", np.cumsum(pca.explained_variance_ratio_))\n",
    "print(\"Components shape:\", pca.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Data in first 2 principal components\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **PCA(n_components=k)**; **fit_transform** on (centered/scaled) data; **components_** are the principal directions.\n",
    "- **explained_variance_ratio_** tells you how much variance each PC captures; use it to choose k.\n",
    "- **inverse_transform** reconstructs data in original space (with loss if k < full rank)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning2fly-sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
