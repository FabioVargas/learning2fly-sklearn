{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Random Forests\n",
    "\n",
    "**Purpose:** Learn and revise **Random Forests** in Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Random Forest?\n",
    "\n",
    "A **random forest** is an **ensemble** of many **decision trees**. Each tree is trained on a **bootstrap sample** of the data and at each split considers only a **random subset of features**. Predictions are made by **averaging** (regression) or **majority voting** (classification).\n",
    "\n",
    "- **Bootstrap:** Each tree sees a random sample of rows (with replacement); about 1/3 of rows are \"out-of-bag\" per tree.\n",
    "- **Feature randomness:** At each split, only \\( m \\) features are tried (e.g. \\( \\sqrt{p} \\) for classification); reduces correlation between trees.\n",
    "\n",
    "**Key idea:** Averaging many noisy but diverse trees reduces variance and overfitting while keeping the flexibility of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts to Remember\n",
    "\n",
    "| Concept | Description |\n",
    "|--------|-------------|\n",
    "| **n_estimators** | Number of trees; more = often better (with diminishing returns), slower. |\n",
    "| **max_features** | Features considered per split; smaller = more diversity, less overfitting. |\n",
    "| **Out-of-bag (OOB)** | Set **oob_score=True** to get an unbiased estimate without a separate validation set. |\n",
    "| **When to use** | Strong default for tabular data; robust, handles non-linearity, gives feature importances. |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.randn(300, 4)\n",
    "y = (X[:, 0]**2 + X[:, 1] > 0.5).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, oob_score=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy (test):\", accuracy_score(y_test, y_pred))\n",
    "print(\"OOB score:\", model.oob_score_)\n",
    "print(\"Feature importances:\", model.feature_importances_)\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **RandomForestClassifier** / **RandomForestRegressor**; tune **n_estimators**, **max_depth**, **max_features**.\n",
    "- **feature_importances_** aggregates importance across trees (based on impurity decrease).\n",
    "- Use **oob_score=True** for a quick validation estimate without cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.10.0" }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
